{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix\nimport multiprocessing\nimport copy\nfrom tqdm import tqdm\nimport time\nfrom math import sqrt\nfrom skimage import io \nfrom PIL import Image, ImageFilter\nimport random\nfrom keras.preprocessing.image import ImageDataGenerator \nimport torchvision\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:05:03.436672Z","iopub.execute_input":"2023-04-30T14:05:03.437406Z","iopub.status.idle":"2023-04-30T14:05:14.375544Z","shell.execute_reply.started":"2023-04-30T14:05:03.437368Z","shell.execute_reply":"2023-04-30T14:05:14.374407Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Model Testing\n\n","metadata":{}},{"cell_type":"markdown","source":"In this section we will be testing the CNN as defined in section 4 under two different modifications to its input. Firstly, we will investigate the effect of the image pre-processing in section 3 by training an initial CNN (model) on the unprocessed data, and an alternative CNN (new_model) on the new, augmented data. \n\nSecondly, we will attempt to test our hypothesis as proposed in section 4: that the angle that the image is taken from is an important factor in the performance of the model. To test this, we will attempt to classify the test set into three categories: 'Side-On', 'Back', and 'Top-Down'. Then we will run the CNN on each category of data to see if there is any noticable difference in prediction accuracy. ","metadata":{}},{"cell_type":"markdown","source":"We begin by loading in both the original and preprocessed training data, and the original test data.\n\n*Note: We will be loading in a lot of data and training multiple neural networks in this section, so to conserve memory we now resize the images to 100x100.*","metadata":{}},{"cell_type":"code","source":"# LOADING THE DATA\n\n#Defining our four class names\nclass_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\n#No. of classes\nnb_classes = len(class_names)\n\n#Image pixel width x height\nIMAGE_SIZE = (100, 100)\n\n#Loading Training and Testing folders\ndatasets = ['/kaggle/input/brain-tumor-mri-dataset/Training','/kaggle/input/processed-brain-data/Training','/kaggle/input/brain-tumor-mri-dataset/Testing']\noutput = []\n#Perform for both Training and Testing\nfor dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        #Searching through each folder corresponding to a class\n        for folder in class_names:\n            label = class_names_label[folder]\n            \n            #Searching through every image within the folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Getting path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Opening and resizing image\n                image = cv2.imread(img_path)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its label to the output\n                images.append(image)\n                labels.append(label)\n        \n        #Convert to array\n        images = np.array(images)\n        labels = np.array(labels)   \n        \n        output.append((images, labels))\n\n#Define training and testing images and labels\n#(train_images, train_labels) = output\n#train_images = output[0][0]\n#train_labels = output[0][1]\n(train_images, train_labels), (new_train_images, new_train_labels), (test_images, test_labels) = output","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:05:14.377471Z","iopub.execute_input":"2023-04-30T14:05:14.378151Z","iopub.status.idle":"2023-04-30T14:09:25.695032Z","shell.execute_reply.started":"2023-04-30T14:05:14.378120Z","shell.execute_reply":"2023-04-30T14:09:25.694002Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loading /kaggle/input/brain-tumor-mri-dataset/Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1321/1321 [00:08<00:00, 157.76it/s]\n100%|██████████| 1339/1339 [00:09<00:00, 145.16it/s]\n100%|██████████| 1595/1595 [00:09<00:00, 162.29it/s]\n100%|██████████| 1457/1457 [00:10<00:00, 136.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading /kaggle/input/processed-brain-data/Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5349/5349 [00:46<00:00, 115.65it/s]\n100%|██████████| 5411/5411 [00:47<00:00, 114.30it/s]\n100%|██████████| 6435/6435 [00:56<00:00, 113.78it/s]\n100%|██████████| 5893/5893 [00:51<00:00, 114.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading /kaggle/input/brain-tumor-mri-dataset/Testing\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 300/300 [00:01<00:00, 154.05it/s]\n100%|██████████| 306/306 [00:01<00:00, 153.10it/s]\n100%|██████████| 405/405 [00:01<00:00, 204.07it/s]\n100%|██████████| 300/300 [00:02<00:00, 149.32it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then to label the angles of the testing data, we use the images outputted by section 6","metadata":{}},{"cell_type":"code","source":"# LOADING THE DATA\n\n#Defining our four class names\nclass_names = ['back', 'sideon', 'topdown']\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\n#No. of classes\nnb_classes = len(class_names)\n\n#Image pixel width x height\nIMAGE_SIZE = (100, 100)\n\n#Loading Training and Testing folders\ndatasets = ['/kaggle/input/shreyas-weird-images/Train and Output Combined']\noutput = []\n#Perform for both Training and Testing\nfor dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        #Searching through each folder corresponding to a class\n        for folder in class_names:\n            label = class_names_label[folder]\n            \n            #Searching through every image within the folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Getting path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Opening and resizing image\n                image = cv2.imread(img_path)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its label to the output\n                images.append(image)\n                labels.append(label)\n        \n        #Convert to array\n        images = np.array(images)\n        labels = np.array(labels)   \n        \n        output.append((images, labels))\n\n#Define training and testing images and labels\n#(train_images, train_labels) = output\nangle_train_images = output[0][0]\nangle_train_labels = output[0][1]","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:09:25.696729Z","iopub.execute_input":"2023-04-30T14:09:25.697137Z","iopub.status.idle":"2023-04-30T14:12:09.939719Z","shell.execute_reply.started":"2023-04-30T14:09:25.697096Z","shell.execute_reply":"2023-04-30T14:12:09.938633Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading /kaggle/input/shreyas-weird-images/Train and Output Combined\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3610/3610 [00:29<00:00, 121.21it/s]\n100%|██████████| 5694/5694 [00:49<00:00, 114.37it/s]\n100%|██████████| 9041/9041 [01:22<00:00, 109.12it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We begin by defining both our CNNs in exactly the same way, as in section 4, and then train one off the initial data and one off the preproccesed data.","metadata":{}},{"cell_type":"code","source":"#BUILDING INITIAL MODEL\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n])\n\n#Compiling model\nmodel.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:12:09.942733Z","iopub.execute_input":"2023-04-30T14:12:09.943138Z","iopub.status.idle":"2023-04-30T14:12:14.817196Z","shell.execute_reply.started":"2023-04-30T14:12:09.943100Z","shell.execute_reply":"2023-04-30T14:12:14.816111Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#TRAINING CNN\nmodel.fit(train_images, train_labels, batch_size=55, epochs=20, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:12:14.818872Z","iopub.execute_input":"2023-04-30T14:12:14.819251Z","iopub.status.idle":"2023-04-30T14:12:57.138593Z","shell.execute_reply.started":"2023-04-30T14:12:14.819213Z","shell.execute_reply":"2023-04-30T14:12:57.137433Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/20\n84/84 [==============================] - 8s 12ms/step - loss: 6.6521 - accuracy: 0.6518 - val_loss: 1.2344 - val_accuracy: 0.5171\nEpoch 2/20\n84/84 [==============================] - 1s 9ms/step - loss: 0.3045 - accuracy: 0.8928 - val_loss: 0.9069 - val_accuracy: 0.7174\nEpoch 3/20\n84/84 [==============================] - 1s 9ms/step - loss: 0.1732 - accuracy: 0.9396 - val_loss: 0.8006 - val_accuracy: 0.7638\nEpoch 4/20\n84/84 [==============================] - 1s 9ms/step - loss: 0.0744 - accuracy: 0.9764 - val_loss: 1.0140 - val_accuracy: 0.7017\nEpoch 5/20\n84/84 [==============================] - 1s 10ms/step - loss: 0.0439 - accuracy: 0.9882 - val_loss: 1.0642 - val_accuracy: 0.7043\nEpoch 6/20\n84/84 [==============================] - 1s 9ms/step - loss: 0.0825 - accuracy: 0.9779 - val_loss: 0.6947 - val_accuracy: 0.8180\nEpoch 7/20\n84/84 [==============================] - 1s 11ms/step - loss: 0.0988 - accuracy: 0.9698 - val_loss: 0.7306 - val_accuracy: 0.8381\nEpoch 8/20\n84/84 [==============================] - 1s 11ms/step - loss: 0.0273 - accuracy: 0.9912 - val_loss: 0.8915 - val_accuracy: 0.8233\nEpoch 9/20\n84/84 [==============================] - 1s 9ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.8967 - val_accuracy: 0.8250\nEpoch 10/20\n84/84 [==============================] - 1s 10ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.9715 - val_accuracy: 0.8206\nEpoch 11/20\n84/84 [==============================] - 1s 9ms/step - loss: 8.1750e-04 - accuracy: 1.0000 - val_loss: 0.9919 - val_accuracy: 0.8241\nEpoch 12/20\n84/84 [==============================] - 1s 10ms/step - loss: 5.1312e-04 - accuracy: 1.0000 - val_loss: 0.9970 - val_accuracy: 0.8390\nEpoch 13/20\n84/84 [==============================] - 1s 10ms/step - loss: 3.7929e-04 - accuracy: 1.0000 - val_loss: 1.0464 - val_accuracy: 0.8346\nEpoch 14/20\n84/84 [==============================] - 1s 9ms/step - loss: 2.9477e-04 - accuracy: 1.0000 - val_loss: 1.0780 - val_accuracy: 0.8320\nEpoch 15/20\n84/84 [==============================] - 1s 9ms/step - loss: 2.4186e-04 - accuracy: 1.0000 - val_loss: 1.0763 - val_accuracy: 0.8373\nEpoch 16/20\n84/84 [==============================] - 1s 9ms/step - loss: 1.9938e-04 - accuracy: 1.0000 - val_loss: 1.0959 - val_accuracy: 0.8381\nEpoch 17/20\n84/84 [==============================] - 1s 10ms/step - loss: 1.6756e-04 - accuracy: 1.0000 - val_loss: 1.0978 - val_accuracy: 0.8425\nEpoch 18/20\n84/84 [==============================] - 1s 10ms/step - loss: 1.4550e-04 - accuracy: 1.0000 - val_loss: 1.1280 - val_accuracy: 0.8373\nEpoch 19/20\n84/84 [==============================] - 1s 9ms/step - loss: 1.2559e-04 - accuracy: 1.0000 - val_loss: 1.1248 - val_accuracy: 0.8451\nEpoch 20/20\n84/84 [==============================] - 1s 9ms/step - loss: 1.1149e-04 - accuracy: 1.0000 - val_loss: 1.1478 - val_accuracy: 0.8399\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7acd179dd790>"},"metadata":{}}]},{"cell_type":"code","source":"#BUILDING INITIAL MODEL\nnew_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n])\n\n#Compiling model\nnew_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:12:57.140145Z","iopub.execute_input":"2023-04-30T14:12:57.143293Z","iopub.status.idle":"2023-04-30T14:12:57.209300Z","shell.execute_reply.started":"2023-04-30T14:12:57.143260Z","shell.execute_reply":"2023-04-30T14:12:57.208274Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#TRAINING CNN\nnew_model.fit(new_train_images, new_train_labels, batch_size=55, epochs=20, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:12:57.210842Z","iopub.execute_input":"2023-04-30T14:12:57.211195Z","iopub.status.idle":"2023-04-30T14:13:58.130121Z","shell.execute_reply.started":"2023-04-30T14:12:57.211158Z","shell.execute_reply":"2023-04-30T14:13:58.128995Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/20\n336/336 [==============================] - 5s 11ms/step - loss: 3.4826 - accuracy: 0.7235 - val_loss: 1.2558 - val_accuracy: 0.5808\nEpoch 2/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.3517 - accuracy: 0.8652 - val_loss: 1.0153 - val_accuracy: 0.6698\nEpoch 3/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.2127 - accuracy: 0.9225 - val_loss: 1.0693 - val_accuracy: 0.6890\nEpoch 4/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.1296 - accuracy: 0.9527 - val_loss: 1.2249 - val_accuracy: 0.7003\nEpoch 5/20\n336/336 [==============================] - 3s 9ms/step - loss: 0.0882 - accuracy: 0.9712 - val_loss: 1.8788 - val_accuracy: 0.6176\nEpoch 6/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0630 - accuracy: 0.9793 - val_loss: 2.0209 - val_accuracy: 0.6589\nEpoch 7/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 1.7669 - val_accuracy: 0.7031\nEpoch 8/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0539 - accuracy: 0.9831 - val_loss: 2.0752 - val_accuracy: 0.6687\nEpoch 9/20\n336/336 [==============================] - 3s 9ms/step - loss: 0.0503 - accuracy: 0.9839 - val_loss: 1.7895 - val_accuracy: 0.7074\nEpoch 10/20\n336/336 [==============================] - 3s 9ms/step - loss: 0.0597 - accuracy: 0.9818 - val_loss: 2.1891 - val_accuracy: 0.7027\nEpoch 11/20\n336/336 [==============================] - 3s 9ms/step - loss: 0.0691 - accuracy: 0.9781 - val_loss: 2.1685 - val_accuracy: 0.7217\nEpoch 12/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 2.0287 - val_accuracy: 0.7427\nEpoch 13/20\n336/336 [==============================] - 3s 9ms/step - loss: 0.0252 - accuracy: 0.9926 - val_loss: 2.2847 - val_accuracy: 0.7300\nEpoch 14/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0429 - accuracy: 0.9862 - val_loss: 2.4137 - val_accuracy: 0.7137\nEpoch 15/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 2.9445 - val_accuracy: 0.6165\nEpoch 16/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0366 - accuracy: 0.9891 - val_loss: 2.3625 - val_accuracy: 0.7007\nEpoch 17/20\n336/336 [==============================] - 3s 9ms/step - loss: 0.0144 - accuracy: 0.9960 - val_loss: 2.3737 - val_accuracy: 0.7451\nEpoch 18/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0087 - accuracy: 0.9978 - val_loss: 2.6460 - val_accuracy: 0.6951\nEpoch 19/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0351 - accuracy: 0.9904 - val_loss: 2.4092 - val_accuracy: 0.7289\nEpoch 20/20\n336/336 [==============================] - 3s 8ms/step - loss: 0.0595 - accuracy: 0.9835 - val_loss: 2.6490 - val_accuracy: 0.6986\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7acd14d6a150>"},"metadata":{}}]},{"cell_type":"markdown","source":"Then as a method for labelling the angles of the test data, we will first train a CNN model on the data from section 6, where each image is sorted into a folder by angle. We then use this trained angle model to predict the angle of each test image, and then sort the test images into the 3 angles using this prediction.","metadata":{}},{"cell_type":"code","source":"#BUILDING INITIAL MODEL\nangle_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n])\n\n#Compiling model\nangle_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:13:58.135028Z","iopub.execute_input":"2023-04-30T14:13:58.137861Z","iopub.status.idle":"2023-04-30T14:13:58.251787Z","shell.execute_reply.started":"2023-04-30T14:13:58.137812Z","shell.execute_reply":"2023-04-30T14:13:58.250571Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#TRAINING CNN\nangle_model.fit(angle_train_images, angle_train_labels, batch_size=55, epochs=20, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:13:58.253370Z","iopub.execute_input":"2023-04-30T14:13:58.254127Z","iopub.status.idle":"2023-04-30T14:14:47.684689Z","shell.execute_reply.started":"2023-04-30T14:13:58.254086Z","shell.execute_reply":"2023-04-30T14:14:47.683553Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/20\n267/267 [==============================] - 4s 11ms/step - loss: 7.6939 - accuracy: 0.8448 - val_loss: 0.3125 - val_accuracy: 0.8812\nEpoch 2/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.2213 - accuracy: 0.9210 - val_loss: 0.2387 - val_accuracy: 0.9185\nEpoch 3/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.1552 - accuracy: 0.9444 - val_loss: 0.2717 - val_accuracy: 0.9191\nEpoch 4/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.1129 - accuracy: 0.9641 - val_loss: 0.4113 - val_accuracy: 0.8994\nEpoch 5/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0817 - accuracy: 0.9753 - val_loss: 0.2454 - val_accuracy: 0.9365\nEpoch 6/20\n267/267 [==============================] - 2s 8ms/step - loss: 0.0606 - accuracy: 0.9817 - val_loss: 0.4317 - val_accuracy: 0.9158\nEpoch 7/20\n267/267 [==============================] - 2s 8ms/step - loss: 0.0621 - accuracy: 0.9858 - val_loss: 0.3663 - val_accuracy: 0.9101\nEpoch 8/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0433 - accuracy: 0.9891 - val_loss: 0.3859 - val_accuracy: 0.9201\nEpoch 9/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0454 - accuracy: 0.9894 - val_loss: 0.3638 - val_accuracy: 0.9234\nEpoch 10/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0462 - accuracy: 0.9873 - val_loss: 0.3889 - val_accuracy: 0.9321\nEpoch 11/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0435 - accuracy: 0.9894 - val_loss: 0.6000 - val_accuracy: 0.8981\nEpoch 12/20\n267/267 [==============================] - 2s 8ms/step - loss: 0.0566 - accuracy: 0.9856 - val_loss: 0.5316 - val_accuracy: 0.9147\nEpoch 13/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0321 - accuracy: 0.9909 - val_loss: 0.4458 - val_accuracy: 0.9248\nEpoch 14/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0310 - accuracy: 0.9912 - val_loss: 0.5539 - val_accuracy: 0.9201\nEpoch 15/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0430 - accuracy: 0.9879 - val_loss: 0.4906 - val_accuracy: 0.9212\nEpoch 16/20\n267/267 [==============================] - 2s 8ms/step - loss: 0.0346 - accuracy: 0.9906 - val_loss: 0.5023 - val_accuracy: 0.9177\nEpoch 17/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0372 - accuracy: 0.9909 - val_loss: 0.4078 - val_accuracy: 0.9267\nEpoch 18/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0412 - accuracy: 0.9892 - val_loss: 0.3762 - val_accuracy: 0.9237\nEpoch 19/20\n267/267 [==============================] - 2s 9ms/step - loss: 0.0191 - accuracy: 0.9952 - val_loss: 0.4338 - val_accuracy: 0.9308\nEpoch 20/20\n267/267 [==============================] - 2s 8ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.4480 - val_accuracy: 0.9180\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7acd14897e50>"},"metadata":{}}]},{"cell_type":"code","source":"test_angle_predictions = angle_model.predict(test_images)\ntest_angle_pred_labels = np.argmax(test_angle_predictions, axis =1)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:14:47.688005Z","iopub.execute_input":"2023-04-30T14:14:47.688356Z","iopub.status.idle":"2023-04-30T14:14:48.035137Z","shell.execute_reply.started":"2023-04-30T14:14:47.688327Z","shell.execute_reply":"2023-04-30T14:14:48.034048Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"41/41 [==============================] - 0s 3ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"test_sideon_images=[]\ntest_sideon_labels=[]\n\ntest_topdown_images=[]\ntest_topdown_labels=[]\n\ntest_back_images=[]\ntest_back_labels=[]\n\nfor i in range(len(test_images)):\n    if (test_angle_pred_labels[i]==0):\n        test_back_images.append(test_images[i])\n        test_back_labels.append(test_labels[i])\n    if (test_angle_pred_labels[i]==1):\n        test_sideon_images.append(test_images[i])\n        test_sideon_labels.append(test_labels[i])\n    if (test_angle_pred_labels[i]==2):\n        test_topdown_images.append(test_images[i])\n        test_topdown_labels.append(test_labels[i])\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:14:48.038286Z","iopub.execute_input":"2023-04-30T14:14:48.038901Z","iopub.status.idle":"2023-04-30T14:14:48.047961Z","shell.execute_reply.started":"2023-04-30T14:14:48.038859Z","shell.execute_reply":"2023-04-30T14:14:48.046769Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Finally we are in a position to evaluate the performance of each of these models. First we calculate the accuracy of both 'model' and 'new_model' on the entire test data. Here we can see that the accuracy is improved by the new_model, suggesting that augmenting the images as in the preprocessing stage helped the model train better in differentiating between the image classes.","metadata":{}},{"cell_type":"code","source":"#CALCULATING TEST PREDICTION\ntest_loss = model.evaluate(test_images, test_labels)\nnew_test_loss = new_model.evaluate(test_images, test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:14:48.049793Z","iopub.execute_input":"2023-04-30T14:14:48.050173Z","iopub.status.idle":"2023-04-30T14:14:48.540663Z","shell.execute_reply.started":"2023-04-30T14:14:48.050135Z","shell.execute_reply":"2023-04-30T14:14:48.539631Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"41/41 [==============================] - 0s 3ms/step - loss: 0.6054 - accuracy: 0.9161\n41/41 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.9458\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next we evaluate the new model on the test data separated by angle. As we can see, the predictive accuracy given below seems to give evidence to our hypothesis that there is a great disparity between the performance of the model on 'Top-Down' images relative to any other image.\n\nWhilst the lack of interpretability in the CNN makes it difficult to pinpoint exactly why this may be, an intuitive suggestion is the same one as in section 4: that the 'Top-Down' images contain less artifacts within the images. For instance, most of the top down images, especially in the 'notumour' folder, are simply a relatively undetailed grey circle for the brain with a very identifiable white circle surrounding it. However for the 'Side-On' and 'Back' images, not only is the overall shape of the head inconsistent, we now have to deal with other objects such as noses, eye sockets, jawbones and teeth etc. ","metadata":{}},{"cell_type":"code","source":"sideon_test_loss = new_model.evaluate(np.array(test_sideon_images), np.array(test_sideon_labels))\ntopdown_test_loss = new_model.evaluate(np.array(test_topdown_images), np.array(test_topdown_labels))\nback_test_loss = new_model.evaluate(np.array(test_back_images), np.array(test_back_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-30T14:14:48.542611Z","iopub.execute_input":"2023-04-30T14:14:48.542987Z","iopub.status.idle":"2023-04-30T14:14:49.033192Z","shell.execute_reply.started":"2023-04-30T14:14:48.542949Z","shell.execute_reply":"2023-04-30T14:14:49.031982Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"10/10 [==============================] - 0s 5ms/step - loss: 0.7406 - accuracy: 0.8993\n24/24 [==============================] - 0s 4ms/step - loss: 0.2552 - accuracy: 0.9763\n8/8 [==============================] - 0s 4ms/step - loss: 0.5912 - accuracy: 0.9098\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To conclude then, we can have seen both the image augmentation modification and the identifying of image angle both have a positive effect on the performance of our neural network. \n\nAs an extension, we could look at how to best mitigate the problem of differences in performance between angles. So far we have shown the problem exists, and we can give different levels of uncertainty to a prediction based on its angle, but no actual solution of how to improve the model. To do this we suggest predicting the angle of every image in the training set using the angle CNN (angle_model), and somehow incorporating these new labels into the CNN to retrain the model on both the images and angle labels simultaneously.","metadata":{}}]}